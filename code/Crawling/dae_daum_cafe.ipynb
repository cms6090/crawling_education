{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:\\\\Users\\\\qkfka\\\\Desktop\\\\dd\\\\cafe\\\\' # 파일 경로 바꿀것\n",
    "path = file_path + 'chromedriver.exe' #크롬드라이버\n",
    "driver = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gett(www):\n",
    "    driver.get('{}'.format(www))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "\n",
    "def urll():\n",
    "    url_list.clear()\n",
    "    title_list.clear()\n",
    "    try :\n",
    "        for j in range(0,100):\n",
    "            for i in range(0,10):\n",
    "                try : \n",
    "                    title = driver.find_element_by_xpath('//*[@id=\"cafeResultUL\"]/li[{}]/div[2]/div/div[1]/a'.format(i+1)).text\n",
    "                except : \n",
    "                    title = driver.find_element_by_xpath('//*[@id=\"cafeResultUL\"]/li[{}]/div/div/div[1]/a'.format(i+1)).text\n",
    "\n",
    "                try :\n",
    "                    for_url = driver.find_element_by_xpath('//*[@id=\"cafeResultUL\"]/li[{}]/div[2]/div/div[1]/a'.format(i+1))\n",
    "                    url = for_url.get_attribute('href')\n",
    "                except:\n",
    "                    for_url = driver.find_element_by_xpath('//*[@id=\"cafeResultUL\"]/li[{}]/div/div/div[1]/a'.format(i+1))\n",
    "                    url = for_url.get_attribute('href')\n",
    "\n",
    "                title_list.append(title)\n",
    "                url_list.append(url)\n",
    "                time.sleep(0.5)\n",
    "            if j%10 ==0 :\n",
    "                print(j)\n",
    "            try:\n",
    "                driver.find_element_by_xpath('//*[@id=\"pagingArea\"]/span/span[3]/a').click()\n",
    "            except:\n",
    "                pass\n",
    "    except : \n",
    "        print(\"완료 \")\n",
    "        print(j*10 + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "\n",
    "def tab(url_list):\n",
    "    doc_list.clear()\n",
    "    for i in range(len(url_list)):\n",
    "        url_path = url_list[i]\n",
    "        driver.switch_to_window(driver.window_handles[0]) # 탭 0번 전환\n",
    "        driver.execute_script(\"window.open('{}')\".format(url_path))\n",
    "        driver.switch_to_window(driver.window_handles[1]) # 탭 1번 전환 \n",
    "        time.sleep(5)\n",
    "\n",
    "        driver.switch_to_frame(0) # 첫번째 iframe\n",
    "        html = driver.page_source\n",
    "        soup = BS(html, 'html.parser')\n",
    "        a = soup.find('div', class_='tx-content-container').get_text() \n",
    "        a = a.replace(\"\\t\", \"\")\n",
    "        doc_list.append(a.replace(\"\\n\", \"\"))\n",
    "        driver.switch_to_default_content()\n",
    "        driver.close()\n",
    "        time.sleep(3)\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "\n",
    "for j in range(0,8):\n",
    "    for i in range(1,16):\n",
    "        title = driver.find_element(By.XPATH,'//*[@id=\"twcColl\"]/div[2]/c-container/c-card[{}]/div/c-doc-web/div/div[1]'.format(i)).text\n",
    "        title2 = driver.find_element(By.XPATH, '//*[@id=\"twcColl\"]/div[2]/c-container/c-card[{}]/div/c-doc-web/div/div[1]/c-title'.format(i))\n",
    "        url = title2.get_attribute('data-href')\n",
    "    \n",
    "        title_list.append(title)\n",
    "        url_list.append(url)\n",
    "    driver.find_element(By.CLASS_NAME, 'btn_next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(query_txt): #파일 저장\n",
    "    raw_data = pd.DataFrame()\n",
    "    raw_data['title'] = title_list\n",
    "    raw_data['doc'] = doc_list\n",
    "    raw_data['url'] = url_list\n",
    "    raw_data['ch'] = 'daum'\n",
    "    raw_data['ch2'] = 'cafe'\n",
    "    \n",
    "    \n",
    "    f = open(file_path + \"{}.pkl\".format(query_txt), \"wb\")\n",
    "    pickle.dump(raw_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gett('https://search.daum.net/search?w=cafe&nil_search=btn&DA=STC&enc=utf8&ASearchType=1&lpp=10&rlang=0&q=대전 지역경제&sd=20180101000000&ed=20201231235959&period=u')\n",
    "urll()\n",
    "len(url_list)\n",
    "tab(url_list)\n",
    "save_pkl('daum_cafe_1')\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
