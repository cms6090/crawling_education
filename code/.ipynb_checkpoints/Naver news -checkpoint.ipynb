{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 경로설정\n",
    "file_path = 'C:\\\\Chrome\\\\'\n",
    "path = file_path + 'chromedriver.exe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검색어 및 검색 기간 설정\n",
    "query_txt = '대전 지역경제' # 검색어\n",
    "start_date = '20180101' #시작일\n",
    "end_date = '20201231' #종료일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#드라이버로 네이버 열기\n",
    "driver = webdriver.Chrome(path) \n",
    "driver.get('https://www.naver.com')\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_txt):\n",
    "    \n",
    "    #검색\n",
    "    element = driver.find_element_by_id('query')\n",
    "    element.send_keys(query_txt) \n",
    "    element.submit() \n",
    "    \n",
    "    #카페 크롤링\n",
    "    driver.find_element_by_link_text(\"뉴스\").click() #카페라고 설정되어진 링크 클릭\n",
    "    time.sleep(3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setting():\n",
    "    #검색옵션 설정하기\n",
    "    driver.find_element_by_xpath('//*[@id=\"search_option_button\"]').click() #검색옵션\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div/ul/li[1]/a').click() #정렬\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div/ul/li[1]/div/ul/li[1]/a').click() #관련도\n",
    "    time.sleep(3)\n",
    "     \n",
    "    #기간\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div/ul/li[2]/a').click() #기간 버튼 누르기\n",
    "    time.sleep(3)\n",
    "    print(\"ok\") \n",
    "    \n",
    "    #시작일 설정\n",
    "    s_date = driver.find_element_by_xpath('//*[@id=\"news_input_period_begin\"]')  #시작일 버튼 찾기\n",
    "    time.sleep(3)\n",
    "\n",
    "    s_date.click() #시작일 버튼 클릭\n",
    "    time.sleep(5)\n",
    "\n",
    "    s_date.send_keys(start_date) #시작일 입력\n",
    "\n",
    "    #종료일 설정\n",
    "    e_date = driver.find_element_by_xpath('//*[@id=\"news_input_period_end\"]') #종료일 버튼 찾기(절대경로 복사)\n",
    "    time.sleep(3) \n",
    "\n",
    "    e_date.click()  #종료일 버튼 클릭\n",
    "    time.sleep(5)\n",
    "\n",
    "    e_date.send_keys(end_date) #종료일 입력\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_class_name(\"tx\").click() #적용 버튼 누르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "def urll(): # title, url 뽑는 함수\n",
    "    html = driver.page_source\n",
    "    soup = BS(html, 'html.parser')\n",
    "    title_list.clear()\n",
    "    url_list.clear()\n",
    "    \n",
    "    try:\n",
    "        for j in range(0,100):\n",
    "            titles = soup.select('#main_pack > section > div > div.group_news > ul > li > div > div > a')\n",
    "            \n",
    "            for i in range(0,10):\n",
    "                title_list.append(titles[i].text)\n",
    "                url_list.append(titles[i]['href'])\n",
    "            \n",
    "            driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div[2]/div/a[2]').click()\n",
    "            time.sleep(3)    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "def docl(): # doc 뽑기\n",
    "    from tqdm import tqdm\n",
    "    doc_list.clear()\n",
    "    for i in tqdm(range(len(url_list))):  #(0,len(url_list)):\n",
    "        url_path = url_list[i]\n",
    "        driver.switch_to_window(driver.window_handles[0]) # 탭 0번 전환  #켜진 순서\n",
    "        driver.execute_script(\"window.open('{}')\".format(url_path))  #문서 순서대로 탭 켜짐\n",
    "        driver.switch_to_window(driver.window_handles[1]) # 탭 1번 전환  #켜진 탭으로..\n",
    "        time.sleep(4)\n",
    "\n",
    "        html = driver.page_source #개발자도구\n",
    "        soup = BS(html, 'html.parser') #...\n",
    "        try:\n",
    "            b= soup.select('div > p')\n",
    "            b = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", str(b))\n",
    "        except :\n",
    "            b = 'x'\n",
    "        doc_list.append(b)\n",
    "        driver.close()\n",
    "        time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(qurey_txt): #파일 저장\n",
    "    raw_data = pd.DataFrame() #크롤링한 데이터를 판다스 데이터프레임형식으로 저장\n",
    "    raw_data['title'] = title_list #제목\n",
    "    raw_data['doc'] = doc_list #내용\n",
    "    raw_data['url'] = url_list #url\n",
    "    raw_data['ch'] = 'naver' \n",
    "    raw_data['ch2'] = 'news' \n",
    "    \n",
    "    file_path = 'C:\\\\crawling\\\\data\\\\'\n",
    "    f = open(file_path + \"{}.pkl\".format(qurey_txt), \"wb\")\n",
    "    pickle.dump(raw_data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "search(\"대전+지역경제\")\n",
    "setting() #기간, 관련도 설정\n",
    "urll() #title, url 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]<ipython-input-8-f535aa9be281>:7: DeprecationWarning: use driver.switch_to.window instead\n",
      "  driver.switch_to_window(driver.window_handles[0]) # 탭 0번 전환  #켜진 순서\n",
      "<ipython-input-8-f535aa9be281>:9: DeprecationWarning: use driver.switch_to.window instead\n",
      "  driver.switch_to_window(driver.window_handles[1]) # 탭 1번 전환  #켜진 탭으로..\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:19:40<00:00,  4.78s/it]\n"
     ]
    }
   ],
   "source": [
    "docl() # 본문 내용 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save_pd()\n",
    "save_pkl('naver_news_1')\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
